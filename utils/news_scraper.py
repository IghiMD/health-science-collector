import feedparser
import requests
from datetime import datetime
from utils.logger import logger

SOURCES = {
    "TASR Zdravie": "https://www.teraz.sk/rss/zdravie.rss", 
    "Aktuality Zdravie": "https://www.aktuality.sk/rss/sekcia/zdravotnictvo/",
    "iDNES ZdravÃ­": "https://www.idnes.cz/rss/zdravi"
}

def fetch_feed(url, source_name):
    try:
        logger.info(f"\nğŸ” NaÄÃ­tavam: {source_name.upper()}")
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        feed = feedparser.parse(response.content)
        
        if not feed.entries:
            logger.warning(f"âš ï¸ Å½iadne ÄlÃ¡nky v {source_name}")
            return []

        logger.info(f"ğŸ“¡ Zdroj: {url}")
        logger.info(f"ğŸ“Š PoÄet ÄlÃ¡nkov: {len(feed.entries)}")
        
        articles = []
        for idx, entry in enumerate(feed.entries, 1):
            article = {
                'title': entry.get('title', 'Bez nÃ¡zvu').strip(),
                'url': entry.get('link', '').strip(),
                'content': entry.get('description', '').strip(),
                'source': source_name,
                'published': entry.get('published', 'neuvedenÃ½')
            }
            articles.append(article)
            
            # DetailnÃ½ vÃ½pis kaÅ¾dÃ©ho ÄlÃ¡nku
            logger.info(f"\nğŸ“Œ ÄŒlÃ¡nok {idx}:")
            logger.info(f"   ğŸ·ï¸ Titulok: {article['title']}")
            logger.info(f"   ğŸ”— URL: {article['url']}")
            logger.info(f"   ğŸ“… DÃ¡tum: {article['published']}")
            logger.info(f"   ğŸ“ Obsah: {article['content'][:150]}...")
            logger.info(f"   ğŸ“ DÄºÅ¾ka: {len(article['content'])} znakov")

        logger.info(f"\nâœ… {source_name}: NaÄÃ­tanÃ© {len(articles)} ÄlÃ¡nkov")
        return articles

    except Exception as e:
        logger.error(f"âŒ Chyba v {source_name}: {str(e)}", exc_info=True)
        return []

def scrape_all():
    logger.info("\n" + "="*60)
    logger.info("ğŸŒ SPÃšÅ Å¤AM SCRAPOVANIE ZDROJOV".center(60))
    logger.info("="*60)
    
    all_articles = []
    for name, url in SOURCES.items():
        all_articles.extend(fetch_feed(url, name))
    
    logger.info("\n" + "="*60)
    logger.info(f"ğŸ“Š CELKOM: {len(all_articles)} ÄlÃ¡nkov z {len(SOURCES)} zdrojov".center(60))
    logger.info("="*60)
    return all_articles
